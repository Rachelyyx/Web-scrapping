{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web Scraping Indeed.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Search data scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up wendrivier\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10)\n",
    "driver.set_script_timeout(120)\n",
    "driver.set_page_load_timeout(10)\n",
    "\n",
    "target_url = \"https://www.indeed.com\"\n",
    "\n",
    "driver.get(target_url) # go to indeed.com\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for data scientist\n",
    "input = driver.find_element(By.CSS_SELECTOR, '#text-input-what')\n",
    "time.sleep(5) # 5s\n",
    "input.send_keys('data scientist\\n')\n",
    "time.sleep(3) # 3s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Pages are saved here: /Users/guanliyuan/Desktop/DDR Final Project\n",
      "file saving completed!\n"
     ]
    }
   ],
   "source": [
    "# save the resulting HTML\n",
    "page_html = driver.page_source #get page html\n",
    "\n",
    "# locate the path to store the html result\n",
    "save_location = os.getcwd() # get folder location\n",
    "print('HTML Pages are saved here:',save_location)\n",
    "\n",
    "# setup file name\n",
    "file_name = 'job_datascientists.html'\n",
    "file = os.path.join(save_location, file_name) \n",
    "\n",
    "# get page html\n",
    "page_html = driver.page_source\n",
    "\n",
    "# save file\n",
    "with open(file, 'w+',encoding='utf-8') as f:\n",
    "        f.write(page_html)\n",
    "        f.close()\n",
    "print(\"file saving completed!\")        \n",
    "\n",
    "time.sleep(3) # 3s\n",
    "\n",
    "# close the window\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Search data analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# search for data analyst\n",
    "# set up wendrivier\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10)\n",
    "driver.set_script_timeout(120)\n",
    "driver.set_page_load_timeout(10)\n",
    "\n",
    "target_url = \"https://www.indeed.com\"\n",
    "\n",
    "driver.get(target_url) # go to indeed.com\n",
    "time.sleep(3)\n",
    "\n",
    "input = driver.find_element(By.CSS_SELECTOR, '#text-input-what')\n",
    "time.sleep(5) # 5s\n",
    "input.send_keys('data analyst\\n')\n",
    "time.sleep(3) # 3s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Pages are saved here: /Users/guanliyuan/Desktop/DDR Final Project\n",
      "file saving completed!\n"
     ]
    }
   ],
   "source": [
    "# save the resulting HTML\n",
    "page_html = driver.page_source #get page html\n",
    "\n",
    "# locate the path to store the html result\n",
    "save_location = os.getcwd() # get folder location\n",
    "print('HTML Pages are saved here:',save_location)\n",
    "\n",
    "# setup file name\n",
    "file_name = 'job_dataanalysts.html'\n",
    "file = os.path.join(save_location, file_name) \n",
    "\n",
    "# get page html\n",
    "page_html = driver.page_source\n",
    "\n",
    "# save file\n",
    "with open(file, 'w+',encoding='utf-8') as f:\n",
    "        f.write(page_html)\n",
    "        f.close()\n",
    "print(\"file saving completed!\")        \n",
    "\n",
    "time.sleep(3) # 3s\n",
    "\n",
    "# close the window\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Search data engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for data engineer\n",
    "# set up wendrivier\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10)\n",
    "driver.set_script_timeout(120)\n",
    "driver.set_page_load_timeout(10)\n",
    "\n",
    "target_url = \"https://www.indeed.com\"\n",
    "\n",
    "driver.get(target_url) # go to indeed.com\n",
    "time.sleep(3)\n",
    "\n",
    "input = driver.find_element(By.CSS_SELECTOR, '#text-input-what')\n",
    "time.sleep(5) # 5s\n",
    "input.send_keys('data engineer\\n')\n",
    "time.sleep(3) # 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Pages are saved here: /Users/guanliyuan/Desktop/DDR Final Project\n",
      "file saving completed!\n"
     ]
    }
   ],
   "source": [
    "# save the resulting HTML\n",
    "page_html = driver.page_source #get page html\n",
    "\n",
    "# locate the path to store the html result\n",
    "save_location = os.getcwd() # get folder location\n",
    "print('HTML Pages are saved here:',save_location)\n",
    "\n",
    "# setup file name\n",
    "file_name = 'job_dataengieneers.html'\n",
    "file = os.path.join(save_location, file_name) \n",
    "\n",
    "# get page html\n",
    "page_html = driver.page_source\n",
    "\n",
    "# save file\n",
    "with open(file, 'w+',encoding='utf-8') as f:\n",
    "        f.write(page_html)\n",
    "        f.close()\n",
    "print(\"file saving completed!\")        \n",
    "\n",
    "time.sleep(3) # 3s\n",
    "\n",
    "# close the window\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Organize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Pages are saved here: /Users/guanliyuan/Desktop/DDR Final Project\n"
     ]
    }
   ],
   "source": [
    "# locate the path to store the html result\n",
    "save_location = os.getcwd() # get folder location\n",
    "print('HTML Pages are saved here:',save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job title: Data & AI Strategy Senior Analyst\n",
      "Company: The Cardinal Way\n",
      "$1,598.42 - $1,924.97 a week\n",
      "Full-time\n",
      "Monday to Friday +4\n",
      "Profit sharing\n",
      "\n",
      "Job title: Data Analyst\n",
      "Company: Emory Healthcare\n",
      "$21.63 - $66.53 an hour\n",
      "Full-time\n",
      "\n",
      "Job title: Data Analyst\n",
      "Company: Emory Healthcare\n",
      "\n",
      "Job title: Data Analyst\n",
      "Company: Analytic Vizion\n",
      "\n",
      "Job title: Senior Data Analyst - Customer Experience - Remote\n",
      "Company: LendingPoint LLC\n",
      "\n",
      "Job title: Senior Data Analyst, People Analytics\n",
      "Company: CVR Associates\n",
      "$22 an hour\n",
      "Full-time +1\n",
      "8 hour shift\n",
      "\n",
      "Job title: Data Analyst II\n",
      "Company: Home Depot / THD\n",
      "Pay information not provided\n",
      "\n",
      "Job title: Lead Data Scientist, People Analytics\n",
      "Company: Chick-fil-A, Inc.\n",
      "Pay information not provided\n",
      "\n",
      "Job title: Senior Data Analyst\n",
      "Company: Calendly\n",
      "$103,700 - $140,300 a year\n",
      "\n",
      "Job title: Data Scientist II\n",
      "Company: FIS Global\n",
      "\n",
      "Job title: Data Scientist, Business Intelligence and Data Strategies\n",
      "Company: Northside Hospital\n",
      "Pay information not provided\n",
      "\n",
      "Job title: Data Scientist\n",
      "Company: ServiceMaster\n",
      "\n",
      "Job title: Senior Data Scientist\n",
      "Company: Norfolk Southern Corp\n",
      "Pay information not provided\n",
      "No weekends\n",
      "\n",
      "Job title: Data & AI Strategy Senior Analyst\n",
      "Company: Cox Communications\n",
      "$97,100 - $161,800 a year\n",
      "Full-time\n",
      "\n",
      "Job title: Data Scientist\n",
      "Company: Emory Healthcare\n",
      "\n",
      "Job title: Senior, Data Scientist - Workforce Development\n",
      "Company: SAP\n",
      "Pay information not provided\n",
      "Full-time\n",
      "\n",
      "Job title: Data Scientist\n",
      "Company: Workday\n",
      "$147,700 - $221,500 a year\n",
      "Full-time\n",
      "\n",
      "Job title: Data Engineer\n",
      "Company: NICE\n",
      "\n",
      "Job title: Sr Data Scientist - Digital Experience Platform\n",
      "Company: Booz Allen\n",
      "$58,300 - $133,000 a year\n",
      "\n",
      "Job title: Data Engineer\n",
      "Company: ServiceNow\n",
      "Pay information not provided\n",
      "Full-time\n",
      "\n",
      "Job title: Principal Data Scientist - BCG X & BCG Fed\n",
      "Company: Trinity Technologies\n",
      "\n",
      "Job title: Data Engineer\n",
      "Company: Booz Allen\n",
      "$58,300 - $133,000 a year\n",
      "\n",
      "Job title: Data Engineer\n",
      "Company: Trinity Technologies\n",
      "\n",
      "Job title: Data Visualization Engineer\n",
      "Company: Aperia\n",
      "Pay information not provided\n",
      "Full-time\n",
      "Monday to Friday\n",
      "\n",
      "Job title: Data Engineer\n",
      "Company: Azurity Pharmaceuticals, Inc.\n",
      "\n",
      "Job title: GRO | Sales Effectiveness & Intelligence | Data Engineer - FLEX LOCATION\n",
      "Company: UNITED PARCEL SERVICE\n",
      "$100,420 - $189,720 a year\n",
      "Full-time\n",
      "\n",
      "Job title: Sr. Data Engineer - Remote\n",
      "Company: Optum\n",
      "\n",
      "Job title: Data Engineer\n",
      "Company: TOCA USA\n",
      "\n",
      "Job title: Data Engineer\n",
      "Company: OneSource Regulatory\n",
      "\n",
      "Job title: Data Engineer II - FIS\n",
      "Company: Meridian Cooperative\n",
      "$95,000 - $105,000 a year\n",
      "8 hour shift\n",
      "\n",
      "Job title: Data Engineer\n",
      "Company: Cloudnile\n",
      "\n",
      "Job title: Data Engineer\n",
      "Company: ParkMobile US\n",
      "\n",
      "Job title: Data Engineer\n",
      "Company: HD Supply\n",
      "\n",
      "Job title: Manager, Data Engineer\n",
      "Company: Zenith\n",
      "Pay information not provided\n",
      "\n",
      "Job title: Data Engineer\n",
      "Company: Martin Concrete\n",
      "Pay information not provided\n",
      "Full-time\n",
      "\n",
      "Job title: Senior Data Engineer\n",
      "Company: FanDuel\n",
      "Pay information not provided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(save_location):\n",
    "    # Check if the file ends with .html\n",
    "    if filename.endswith(\".html\"):\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(save_location, filename)\n",
    "        #print(filename)\n",
    "        # Read file to string\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            html = file.read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # define a regular expression for tags contains 'Data Scientist'\n",
    "            role_re = re.compile(r'Data .*')\n",
    "            info_re = re.compile(r'jobMetaDataGroup.*')\n",
    "            \n",
    "            # search for related info\n",
    "            titles = soup.find_all('a', attrs={\"aria-label\": role_re})\n",
    "            companies = soup.find_all('span', attrs={\"data-testid\": 'company-name'})\n",
    "            info = soup.find_all('div', attrs={\"class\": info_re})  \n",
    "\n",
    "                \n",
    "        # print results            \n",
    "        for i in range(len(titles)):\n",
    "            print('Job title: ' + titles[i].text)\n",
    "            print('Company: ' + companies[i].text)\n",
    "            #print(info[i].text)\n",
    "            if 'Pay information not provided' in info[i].text:\n",
    "                        print(info[i].text[:28])\n",
    "            for item in info[i]:\n",
    "                detail = item.find_all('div', attrs={\"data-testid\": \"attribute_snippet_testid\"})\n",
    "                for i in range(len(detail)):\n",
    "                    if detail[i] != None: # some info are not provided\n",
    "                        print(detail[i].text)\n",
    "                #details = None\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import data to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import mysql.connector\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Suppress MySQL warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Function to create SQL database and table\n",
    "def create_sql_table(SQL_DB, SQL_TABLE_CONTRIBUTORS, SQL_TABLE_CONTRIBUTORS_DEF):\n",
    "    try:\n",
    "        # Connect to server\n",
    "        conn = mysql.connector.connect(host='localhost',\n",
    "                                       user='root',\n",
    "                                       password='Yiyx@1129781') ## Insert your sql server password here\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create database if not exists\n",
    "        query = \"CREATE DATABASE IF NOT EXISTS \" + SQL_DB\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # Create table if not exists\n",
    "        query = \"CREATE TABLE IF NOT EXISTS \" + SQL_DB + \".\" + SQL_TABLE_CONTRIBUTORS + \" \" + SQL_TABLE_CONTRIBUTORS_DEF\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    except IOError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SQL database and table names\n",
    "SQL_DB = \"job_database\"\n",
    "SQL_TABLE_CONTRIBUTORS = \"data_jobs\"\n",
    "SQL_TABLE_CONTRIBUTORS_DEF = \"(job_id INT AUTO_INCREMENT PRIMARY KEY, job_title VARCHAR(255), company VARCHAR(255), salary VARCHAR(255), job_type VARCHAR(255), others TEXT)\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQL database and table\n",
    "create_sql_table(SQL_DB, SQL_TABLE_CONTRIBUTORS, SQL_TABLE_CONTRIBUTORS_DEF)\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(save_location):\n",
    "    if filename.endswith(\".html\"):\n",
    "        filepath = os.path.join(save_location, filename)\n",
    "        \n",
    "        # Read file to string\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            html = file.read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # Define regular expressions for tags containing job information\n",
    "            role_re = re.compile(r'Data .*')\n",
    "            info_re = re.compile(r'jobMetaDataGroup.*')\n",
    "            \n",
    "            # Find related job information\n",
    "            titles = soup.find_all('a', attrs={\"aria-label\": role_re})\n",
    "            companies = soup.find_all('span', attrs={\"data-testid\": 'company-name'})\n",
    "            info = soup.find_all('div', attrs={\"class\": info_re})  \n",
    "\n",
    "        # Insert job information into SQL database\n",
    "        try:\n",
    "            # Connect to database\n",
    "            conn = mysql.connector.connect(host='localhost',\n",
    "                                           user='root',\n",
    "                                           password='Yiyx@1129781',  ## Insert your sql server password here\n",
    "                                           database=SQL_DB)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Insert job information into the table\n",
    "            for i in range(len(titles)):\n",
    "                job_title = titles[i].text\n",
    "                company = companies[i].text\n",
    "                pay_information = info[i].text if 'Pay information not provided' in info[i].text else None\n",
    "                \n",
    "                # Parse salary, job type, and other information\n",
    "                salary = None\n",
    "                job_type = None\n",
    "                others = None\n",
    "                if pay_information:\n",
    "                    # Split pay information to extract salary, job type, and others\n",
    "                    parts = pay_information.split(' - ')\n",
    "                    if len(parts) >= 1:\n",
    "                        salary = parts[0].strip()\n",
    "                    if len(parts) >= 2:\n",
    "                        job_type = parts[1].strip()\n",
    "                    if len(parts) >= 3:\n",
    "                        others = '-'.join(parts[2:]).strip()  # Join the remaining parts\n",
    "\n",
    "                # Insert job information into SQL table\n",
    "                query = \"INSERT INTO \" + SQL_TABLE_CONTRIBUTORS + \" (job_title, company, salary, job_type, others) VALUES (%s, %s, %s, %s, %s)\"\n",
    "                cursor.execute(query, (job_title, company, salary, job_type, others))\n",
    "\n",
    "            conn.commit()\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            print(\"MySQL Error: {}\".format(err))\n",
    "\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import data into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Function to get MongoDB database and collection\n",
    "def get_mongodb_collection(mongo_db_name, collection_name):\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient('mongodb://localhost:27017/?retryWrites=true&loadBalanced=false&serverSelectionTimeoutMS=5000&connectTimeoutMS=10000')\n",
    "    db = client[mongo_db_name]\n",
    "    collection = db[collection_name]\n",
    "    return collection\n",
    "\n",
    "# Define MongoDB database and collection names\n",
    "MONGO_DB = \"job_database\"\n",
    "COLLECTION_NAME = \"data_jobs\"\n",
    "\n",
    "# Get MongoDB collection\n",
    "collection = get_mongodb_collection(MONGO_DB, COLLECTION_NAME)\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(save_location):\n",
    "    if filename.endswith(\".html\"):\n",
    "        filepath = os.path.join(save_location, filename)\n",
    "        \n",
    "        # Read file to string\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            html = file.read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # Define regular expressions for tags containing job information\n",
    "            role_re = re.compile(r'Data .*')\n",
    "            info_re = re.compile(r'jobMetaDataGroup.*')\n",
    "            \n",
    "            # Find related job information\n",
    "            titles = soup.find_all('a', attrs={\"aria-label\": role_re})\n",
    "            companies = soup.find_all('span', attrs={\"data-testid\": 'company-name'})\n",
    "            info = soup.find_all('div', attrs={\"class\": info_re})  \n",
    "\n",
    "        # Insert job information into MongoDB collection\n",
    "        for i in range(len(titles)):\n",
    "            job_title = titles[i].text\n",
    "            company = companies[i].text\n",
    "            pay_information = info[i].text if 'Pay information not provided' not in info[i].text else None\n",
    "                \n",
    "            # Parse salary, job type, and other information\n",
    "            document = {\"job_title\": job_title, \"company\": company}\n",
    "            if pay_information:\n",
    "                # Split pay information to extract salary, job type, and others\n",
    "                parts = pay_information.split(' - ')\n",
    "                if len(parts) >= 1:\n",
    "                    document[\"salary\"] = parts[0].strip()\n",
    "                if len(parts) >= 2:\n",
    "                    document[\"job_type\"] = parts[1].strip()\n",
    "                if len(parts) >= 3:\n",
    "                    document[\"others\"] = '-'.join(parts[2:]).strip()  # Join the remaining parts\n",
    "\n",
    "            # Insert job information into MongoDB collection\n",
    "            collection.insert_one(document)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
